#some parameters of network architecture
Com_Encoder: #this part was used for building a common encoder for X1 and Y
  n_cnn: 2
  n_res: 4
  act_function: relu
  Nor_type: Instance
  Z_dim: 64 #dimension number of latent code
  init_weight: normal  # weight init
  pad_style: Ref

Com_Decoder: #this part was used for building a common Decoder for X1 and Y
  n_cnn: 2
  n_res: 4
  act_function: relu
  Nor_type: Instance
  Z_dim: 3 # dimension number of latent code
  pad_style: Ref


Ins_Encoder: # this part was used for building a unique encoder for X2, the instance you want to transform
  n_cnn: 2
  n_res: 2
  Z_dim: 32
  act_function: relu
  Nor_type: None
  pad_style: Ref

Ins_Decoder: # this part was used for building a unique encoder for X2, the instance you want to transform
  n_cnn: 3
  n_res: 2
  Z_dim: 32
  act_function: relu
  Nor_type: None
  pad_style: Ref


Com_Dis:
  n_cnn: 4
  n_res: 0
  act_function: relu
  Nor_type: None
  Z_dim: 64 # dimension number of latent code
  Dis_outputs: 1 #the number of outputs
  pad_style: Ref

Ins_Dis:
  n_cnn: 3
  n_res: 0
  act_function: relu
  Nor_type: None
  Z_dim: 32 # dimension number of latent code
  Dis_outputs: 1 #the number of outputs
  pad_style: Ref

ComGen Optimizer:
  max_iter: 1000000             # maximum number of training iterations
  weight_decay: 0.0001          # weight decay
  beta1: 0.5                    # Adam parameter
  beta2: 0.999                  # Adam parameter
  init: kaiming                 # initialization [gaussian/kaiming/xavier/orthogonal]
  lr: 0.0001                    # initial learning rate
  lr_policy: step               # learning rate scheduler
  step_size: 100000             # how often to decay learning rate
  gamma: 0.5                    # how much to decay learning rate
  gan_w: 1                      # weight of adversarial loss
  recon_x_w: 10                 # weight of image reconstruction loss
  recon_h_w: 0                  # weight of hidden reconstruction loss
  recon_kl_w: 0.01              # weight of KL loss for reconstruction
  recon_x_cyc_w: 10             # weight of cycle consistency loss
  recon_kl_cyc_w: 0.01          # weight of KL loss for cycle consistency

ComDis Optimizer:
  max_iter: 1000000             # maximum number of training iterations
  weight_decay: 0.0001          # weight decay
  beta1: 0.5                    # Adam parameter
  beta2: 0.999                  # Adam parameter
  init: kaiming                 # initialization [gaussian/kaiming/xavier/orthogonal]
  lr: 0.0001                    # initial learning rate
  lr_policy: step               # learning rate scheduler
  step_size: 100000             # how often to decay learning rate
  gamma: 0.5                    # how much to decay learning rate
  gan_w: 1                      # weight of adversarial loss
  recon_x_w: 10                 # weight of image reconstruction loss
  recon_h_w: 0                  # weight of hidden reconstruction loss
  recon_kl_w: 0.01              # weight of KL loss for reconstruction
  recon_x_cyc_w: 10             # weight of cycle consistency loss
  recon_kl_cyc_w: 0.01          # weight of KL loss for cycle consistency
  Dis_loss_weight: 10

lr type: step
step size: 100000

InstanceGen Optimizer:
  max_iter: 1000000             # maximum number of training iterations
  batch_size: 1                 # batch size
  weight_decay: 0.0001          # weight decay
  beta1: 0.5                    # Adam parameter
  beta2: 0.999                  # Adam parameter
  init: kaiming                 # initialization [gaussian/kaiming/xavier/orthogonal]
  lr: 0.0001                    # initial learning rate
  lr_policy: step               # learning rate scheduler
  step_size: 100000             # how often to decay learning rate
  gamma: 0.5                    # how much to decay learning rate
  gan_w: 1                      # weight of adversarial loss
  recon_x_w: 10                 # weight of image reconstruction loss
  recon_h_w: 0                  # weight of hidden reconstruction loss
  recon_kl_w: 0.01              # weight of KL loss for reconstruction
  recon_x_cyc_w: 10             # weight of cycle consistency loss
  recon_kl_cyc_w: 0.01          # weight of KL loss for cycle consistency


InstanceDis Optimizer:
  max_iter: 1000000             # maximum number of training iterations
  batch_size: 1                 # batch size
  weight_decay: 0.0001          # weight decay
  beta1: 0.5                    # Adam parameter
  beta2: 0.999                  # Adam parameter
  init: kaiming                 # initialization [gaussian/kaiming/xavier/orthogonal]
  lr: 0.0001                    # initial learning rate
  lr_policy: step               # learning rate scheduler
  step_size: 100000             # how often to decay learning rate
  gamma: 0.5                    # how much to decay learning rate
  gan_w: 1                      # weight of adversarial loss
  recon_x_w: 10                 # weight of image reconstruction loss
  recon_h_w: 0                  # weight of hidden reconstruction loss
  recon_kl_w: 0.01              # weight of KL loss for reconstruction
  recon_x_cyc_w: 10             # weight of cycle consistency loss
  recon_kl_cyc_w: 0.01          # weight of KL loss for cycle consistency
  Dis_loss_weight: 10
# Dataset
DataSet: # this part provides some options for building your own dataset loader
  img_size: 512
  filepath: /root/Desktop/Pascal/


# optimization options
max_iter: 1000000             # maximum number of training iterations
batch_size: 3              # batch size
weight_decay: 0.0001          # weight decay
beta1: 0.5                    # Adam parameter
beta2: 0.999                  # Adam parameter
init: kaiming                 # initialization [gaussian/kaiming/xavier/orthogonal]
lr: 0.0001                    # initial learning rate
lr_policy: step               # learning rate scheduler
step_size: 1000             # how often to decay learning rate
gamma: 0.5                    # how much to decay learning rate
gan_w: 1                      # weight of adversarial loss
recon_x_w: 10                 # weight of image reconstruction loss
recon_h_w: 0                  # weight of hidden reconstruction loss
recon_kl_w: 0.01              # weight of KL loss for reconstruction
recon_x_cyc_w: 10             # weight of cycle consistency loss
recon_kl_cyc_w: 0.01          # weight of KL loss for cycle consistency
vgg_w: 0                      # weight of domain-invariant perceptual loss

training:
    batch_size: 1
    vis_iter: 1
    max_iter: 10000
    model_save_iter: 200     # How often do you want to save trained models
    log_iter: 1                  # How often do you want to log the training stats
    snap_dir: ã€‚/Data/file/

## test network blocks
input_dim_a: 3
gen:
  dim: 64                     # number of filters in the bottommost layer
  mlp_dim: 256                # number of filters in MLP
  style_dim: 8                # length of style code
  activ: relu                 # activation function [relu/lrelu/prelu/selu/tanh]
  n_downsample: 2             # number of downsampling layers in content encoder
  n_res: 4                    # number of residual blocks in content encoder/decoder
  pad_type: reflect           # padding type [zero/reflect]
dis:
  dim: 64                     # number of filters in the bottommost layer
  norm: none                  # normalization layer [none/bn/in/ln]
  activ: lrelu                # activation function [relu/lrelu/prelu/selu/tanh]
  n_layer: 4                  # number of layers in D
  gan_type: lsgan             # GAN loss [lsgan/nsgan]
  num_scales: 3               # number of scales
  pad_type: reflect           # padding type [zero/reflect]